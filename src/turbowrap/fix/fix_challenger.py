"""
Fix Challenger using Gemini 3 Pro with Thinking mode.

Evaluates fixes generated by Claude and provides feedback.
"""

import asyncio
import json
import logging
import re
from datetime import datetime

from turbowrap.config import get_settings
from turbowrap.fix.models import (
    FixChallengerFeedback,
    FixChallengerStatus,
    FixContext,
    FixIssue,
    FixQualityScores,
)
from turbowrap.llm import load_prompt

logger = logging.getLogger(__name__)

# Thinking budget for fix evaluation (10k tokens)
FIX_CHALLENGER_THINKING_BUDGET = 10000


class GeminiFixChallenger:
    """
    Fix challenger using Gemini 3 Pro with thinking mode.

    Evaluates fixes generated by Claude to ensure quality
    before applying them to the codebase.
    """

    def __init__(
        self,
        model: str = "gemini-3-pro-preview",
        thinking_budget: int = FIX_CHALLENGER_THINKING_BUDGET,
        satisfaction_threshold: float = 95.0,
    ):
        """
        Initialize the fix challenger.

        Args:
            model: Gemini model to use (Pro for better reasoning)
            thinking_budget: Token budget for thinking (0-24576)
            satisfaction_threshold: Required score to approve fix
        """
        try:
            from google import genai
            from google.genai import types
        except ImportError as e:
            raise RuntimeError("google-genai not installed") from e

        settings = get_settings()
        api_key = settings.agents.effective_google_key

        if not api_key:
            raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY required")

        self.client = genai.Client(api_key=api_key)
        self.model = model
        self.thinking_budget = thinking_budget
        self.threshold = satisfaction_threshold
        self._types = types

        # Load challenger system prompt
        try:
            self.system_prompt = load_prompt("fix_challenger")
        except FileNotFoundError:
            logger.warning("fix_challenger.md not found, using default prompt")
            self.system_prompt = self._default_system_prompt()

    async def evaluate(
        self,
        context: FixContext,
        original_content: str,
        fixed_content: str,
        changes_summary: str | None = None,
        iteration: int = 1,
    ) -> FixChallengerFeedback:
        """
        Evaluate a proposed fix.

        Args:
            context: The fix context with issue details
            original_content: Original file content before fix
            fixed_content: Fixed file content
            changes_summary: Summary of changes made
            iteration: Challenger iteration number

        Returns:
            FixChallengerFeedback with evaluation results
        """
        prompt = self._build_evaluation_prompt(
            context, original_content, fixed_content, changes_summary
        )

        # Run in thread pool to not block event loop
        response_text, thinking_content = await asyncio.to_thread(
            self._call_gemini_with_thinking,
            prompt,
        )

        # Parse response
        return self._parse_response(response_text, thinking_content, iteration)

    def _call_gemini_with_thinking(self, prompt: str) -> tuple[str, str | None]:
        """
        Call Gemini with thinking mode enabled.

        Returns:
            Tuple of (response_text, thinking_content)
        """
        try:
            # Configure thinking mode
            config = self._types.GenerateContentConfig(
                thinking_config=self._types.ThinkingConfig(thinking_budget=self.thinking_budget)
            )

            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=config,
            )

            # Extract thinking and response
            thinking_content: str | None = None
            response_text = ""

            # Try to extract thinking from response parts
            if hasattr(response, "candidates") and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, "content") and candidate.content:
                    parts = candidate.content.parts
                    if parts:
                        for part in parts:
                            if hasattr(part, "thought") and part.thought:
                                thinking_content = part.text if hasattr(part, "text") else str(part)
                            elif hasattr(part, "text"):
                                response_text += str(part.text)

            # Fallback to simple text extraction
            if not response_text and hasattr(response, "text"):
                response_text = str(response.text) if response.text else ""

            return response_text, thinking_content

        except Exception as e:
            logger.warning(f"Thinking mode failed, falling back to standard: {e}")
            # Fallback without thinking
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
            )
            text = response.text if hasattr(response, "text") else ""
            return str(text) if text else "", None

    def _default_system_prompt(self) -> str:
        """Default system prompt if fix_challenger.md is not found."""
        return """# Fix Challenger - Quality Evaluation System

You are a meticulous code fix evaluator. Score fixes on 4 dimensions (0-100 each):

## Dimensions

1. **Correctness (40%)**: Does the fix solve the issue? Handle edge cases?
2. **Safety (30%)**: No new bugs/vulnerabilities? No breaking changes?
3. **Minimality (15%)**: Changes focused only on the issue? No scope creep?
4. **Style (15%)**: Matches existing code style? Consistent naming/formatting?

## Scoring

- 90-100: Excellent, ready to apply
- 80-89: Good with minor issues
- 70-79: Acceptable but needs refinement
- <70: Needs rework

Status: APPROVED (>=80), NEEDS_IMPROVEMENT (50-79), REJECTED (<50)

Output ONLY valid JSON with: satisfaction_score, status, quality_scores,
issues_found, improvements_needed, positive_feedback
"""

    def _build_evaluation_prompt(
        self,
        context: FixContext,
        original_content: str,
        fixed_content: str,
        changes_summary: str | None = None,
    ) -> str:
        """Build the evaluation prompt with system context."""
        # Combine system prompt with fix details
        user_prompt = f"""## Fix Evaluation Request

### The Issue Being Fixed

| Field | Value |
|-------|-------|
| **Issue Code** | {context.issue_code} |
| **Title** | {context.title} |
| **Category** | {context.category} |
| **Severity** | {context.severity} |
| **File** | {context.file_path} |
| **Line** | {context.line or "Unknown"} |

**Description**: {context.description}

### Original Problematic Code
```
{context.current_code or "Not specified"}
```

### Suggested Fix (from review)
```
{context.suggested_fix or "Not specified"}
```

---

## Code to Evaluate

### BEFORE (Original File)
```
{original_content[:8000] if len(original_content) > 8000 else original_content}
```
{f"... (truncated, {len(original_content)} chars total)" if len(original_content) > 8000 else ""}

### AFTER (Fixed File)
```
{fixed_content[:8000] if len(fixed_content) > 8000 else fixed_content}
```
{f"... (truncated, {len(fixed_content)} chars total)" if len(fixed_content) > 8000 else ""}

{f"### Changes Summary from Fixer{chr(10)}{changes_summary}" if changes_summary else ""}

---

## Your Evaluation

Apply the scoring rubrics from your system instructions.
Calculate weighted score: (Correctness*0.40) + (Safety*0.30) + (Minimality*0.15) + (Style*0.15)

Return ONLY valid JSON:
```json
{{
  "satisfaction_score": <0-100>,
  "status": "APPROVED|NEEDS_IMPROVEMENT|REJECTED",
  "quality_scores": {{
    "correctness": <0-100>,
    "safety": <0-100>,
    "minimality": <0-100>,
    "style_consistency": <0-100>
  }},
  "issues_found": [
    {{
      "type": "bug|vulnerability|style|logic|performance|breaking",
      "description": "<specific problem>",
      "line": <number or null>,
      "severity": "CRITICAL|HIGH|MEDIUM|LOW",
      "suggestion": "<how to fix>"
    }}
  ],
  "improvements_needed": ["<specific action 1>"],
  "positive_feedback": ["<what was done well>"]
}}
```
"""
        return f"{self.system_prompt}\n\n---\n\n{user_prompt}"

    def _parse_response(
        self,
        response_text: str,
        thinking_content: str | None,
        iteration: int,
    ) -> FixChallengerFeedback:
        """Parse Gemini's response into FixChallengerFeedback."""
        try:
            # Try to extract JSON from response
            json_text = response_text.strip()

            # Handle markdown code blocks
            if "```" in json_text:
                json_match = re.search(r"```(?:json)?\s*(.*?)```", json_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1).strip()

            data = json.loads(json_text)

            # Parse quality scores
            qs_data = data.get("quality_scores", {})
            quality_scores = FixQualityScores(
                correctness=qs_data.get("correctness", 50),
                safety=qs_data.get("safety", 50),
                minimality=qs_data.get("minimality", 50),
                style_consistency=qs_data.get("style_consistency", 50),
            )

            # Parse issues found
            issues_found = []
            for issue_data in data.get("issues_found", []):
                issues_found.append(
                    FixIssue(
                        type=issue_data.get("type", "unknown"),
                        description=issue_data.get("description", ""),
                        line=issue_data.get("line"),
                        severity=issue_data.get("severity", "MEDIUM"),
                        suggestion=issue_data.get("suggestion"),
                    )
                )

            # Determine status
            score = data.get("satisfaction_score", quality_scores.weighted_score)
            status_str = data.get("status", "")

            if status_str:
                try:
                    status = FixChallengerStatus(status_str)
                except ValueError:
                    status = self._score_to_status(score)
            else:
                status = self._score_to_status(score)

            return FixChallengerFeedback(
                iteration=iteration,
                timestamp=datetime.utcnow(),
                satisfaction_score=score,
                threshold=self.threshold,
                status=status,
                quality_scores=quality_scores,
                issues_found=issues_found,
                improvements_needed=data.get("improvements_needed", []),
                positive_feedback=data.get("positive_feedback", []),
                thinking_content=thinking_content,
            )

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse challenger response: {e}")
            # Return minimal feedback on parse error
            return FixChallengerFeedback(
                iteration=iteration,
                satisfaction_score=50,
                threshold=self.threshold,
                status=FixChallengerStatus.NEEDS_IMPROVEMENT,
                quality_scores=FixQualityScores(
                    correctness=50,
                    safety=50,
                    minimality=50,
                    style_consistency=50,
                ),
                improvements_needed=[f"Failed to parse challenger response: {response_text[:200]}"],
            )

    def _score_to_status(self, score: float) -> FixChallengerStatus:
        """Convert satisfaction score to status."""
        if score >= self.threshold:
            return FixChallengerStatus.APPROVED
        if score >= 50:
            return FixChallengerStatus.NEEDS_IMPROVEMENT
        return FixChallengerStatus.REJECTED
