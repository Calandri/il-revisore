"""
Fix Challenger using Gemini 3 Pro with Thinking mode.

Evaluates fixes generated by Claude and provides feedback.
"""

import asyncio
import json
import logging
import re
from datetime import datetime
from typing import Optional

from turbowrap.config import get_settings
from turbowrap.fix.models import (
    FixChallengerFeedback,
    FixChallengerStatus,
    FixContext,
    FixIssue,
    FixQualityScores,
)
from turbowrap.llm import load_prompt

logger = logging.getLogger(__name__)

# Thinking budget for fix evaluation (10k tokens)
FIX_CHALLENGER_THINKING_BUDGET = 10000


class GeminiFixChallenger:
    """
    Fix challenger using Gemini 3 Pro with thinking mode.

    Evaluates fixes generated by Claude to ensure quality
    before applying them to the codebase.
    """

    def __init__(
        self,
        model: str = "gemini-3-pro-preview",
        thinking_budget: int = FIX_CHALLENGER_THINKING_BUDGET,
        satisfaction_threshold: float = 80.0,
    ):
        """
        Initialize the fix challenger.

        Args:
            model: Gemini model to use (Pro for better reasoning)
            thinking_budget: Token budget for thinking (0-24576)
            satisfaction_threshold: Required score to approve fix
        """
        try:
            from google import genai
            from google.genai import types
        except ImportError as e:
            raise RuntimeError("google-genai not installed") from e

        settings = get_settings()
        api_key = settings.agents.effective_google_key

        if not api_key:
            raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY required")

        self.client = genai.Client(api_key=api_key)
        self.model = model
        self.thinking_budget = thinking_budget
        self.threshold = satisfaction_threshold
        self._types = types

        # Load challenger system prompt
        try:
            self.system_prompt = load_prompt("fix_challenger")
        except FileNotFoundError:
            logger.warning("fix_challenger.md not found, using default prompt")
            self.system_prompt = self._default_system_prompt()

    async def evaluate(
        self,
        context: FixContext,
        original_content: str,
        fixed_content: str,
        changes_summary: Optional[str] = None,
        iteration: int = 1,
    ) -> FixChallengerFeedback:
        """
        Evaluate a proposed fix.

        Args:
            context: The fix context with issue details
            original_content: Original file content before fix
            fixed_content: Fixed file content
            changes_summary: Summary of changes made
            iteration: Challenger iteration number

        Returns:
            FixChallengerFeedback with evaluation results
        """
        prompt = self._build_evaluation_prompt(
            context, original_content, fixed_content, changes_summary
        )

        # Run in thread pool to not block event loop
        response_text, thinking_content = await asyncio.to_thread(
            self._call_gemini_with_thinking,
            prompt,
        )

        # Parse response
        feedback = self._parse_response(response_text, thinking_content, iteration)
        return feedback

    def _call_gemini_with_thinking(self, prompt: str) -> tuple[str, Optional[str]]:
        """
        Call Gemini with thinking mode enabled.

        Returns:
            Tuple of (response_text, thinking_content)
        """
        try:
            # Configure thinking mode
            config = self._types.GenerateContentConfig(
                thinking_config=self._types.ThinkingConfig(
                    thinking_budget=self.thinking_budget
                )
            )

            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=config,
            )

            # Extract thinking and response
            thinking_content = None
            response_text = ""

            # Try to extract thinking from response parts
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    for part in candidate.content.parts:
                        if hasattr(part, 'thought') and part.thought:
                            thinking_content = part.text if hasattr(part, 'text') else str(part)
                        elif hasattr(part, 'text'):
                            response_text += part.text

            # Fallback to simple text extraction
            if not response_text and hasattr(response, 'text'):
                response_text = response.text

            return response_text, thinking_content

        except Exception as e:
            logger.warning(f"Thinking mode failed, falling back to standard: {e}")
            # Fallback without thinking
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
            )
            return response.text, None

    def _build_evaluation_prompt(
        self,
        context: FixContext,
        original_content: str,
        fixed_content: str,
        changes_summary: Optional[str] = None,
    ) -> str:
        """Build the evaluation prompt."""
        return f"""# Fix Quality Evaluation

You are evaluating a code fix. Your job is to determine if the fix is correct and safe to apply.

## The Issue Being Fixed

**Code**: {context.issue_code}
**Title**: {context.title}
**Description**: {context.description}
**Category**: {context.category}
**Severity**: {context.severity}
**File**: {context.file_path}
**Line**: {context.line or "Unknown"}

### Original Problematic Code
```
{context.current_code or "Not specified"}
```

### Suggested Fix (from review)
```
{context.suggested_fix or "Not specified"}
```

## The Fix to Evaluate

### Original File
```
{original_content}
```

### Fixed File
```
{fixed_content}
```

{f"### Changes Summary{chr(10)}{changes_summary}" if changes_summary else ""}

## Your Task

Evaluate the fix on these 4 dimensions (0-100 each):

### 1. Correctness (weight: 40%)
- Does the fix actually solve the reported issue?
- Is the logic correct?
- Does it handle edge cases?

### 2. Safety (weight: 30%)
- Does the fix introduce any new bugs?
- Any security vulnerabilities added?
- Any breaking changes to existing behavior?

### 3. Minimality (weight: 15%)
- Is the fix focused on just the issue?
- Any unnecessary changes or over-engineering?
- Is the solution the simplest that works?

### 4. Style Consistency (weight: 15%)
- Does it maintain the existing code style?
- Consistent indentation, naming, etc.?
- Follows the project's conventions?

## Output Format

Return ONLY valid JSON:

```json
{{
  "satisfaction_score": <weighted average 0-100>,
  "status": "APPROVED|NEEDS_IMPROVEMENT|REJECTED",
  "quality_scores": {{
    "correctness": <0-100>,
    "safety": <0-100>,
    "minimality": <0-100>,
    "style_consistency": <0-100>
  }},
  "issues_found": [
    {{
      "type": "bug|vulnerability|style|logic",
      "description": "<what's wrong>",
      "line": <line number or null>,
      "severity": "CRITICAL|HIGH|MEDIUM|LOW",
      "suggestion": "<how to fix>"
    }}
  ],
  "improvements_needed": ["<improvement 1>", "<improvement 2>"],
  "positive_feedback": ["<what was done well>"]
}}
```

## Scoring Guide

- **90-100**: Excellent fix, ready to apply
- **80-89**: Good fix with minor issues
- **70-79**: Acceptable but needs refinement
- **<70**: Fix has problems, needs rework

Status mapping:
- APPROVED: score >= 80
- NEEDS_IMPROVEMENT: 50 <= score < 80
- REJECTED: score < 50

Be fair but rigorous. The fix must not introduce new bugs.
Output ONLY the JSON, no markdown or explanations.
"""

    def _parse_response(
        self,
        response_text: str,
        thinking_content: Optional[str],
        iteration: int,
    ) -> FixChallengerFeedback:
        """Parse Gemini's response into FixChallengerFeedback."""
        try:
            # Try to extract JSON from response
            json_text = response_text.strip()

            # Handle markdown code blocks
            if "```" in json_text:
                json_match = re.search(r"```(?:json)?\s*(.*?)```", json_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1).strip()

            data = json.loads(json_text)

            # Parse quality scores
            qs_data = data.get("quality_scores", {})
            quality_scores = FixQualityScores(
                correctness=qs_data.get("correctness", 50),
                safety=qs_data.get("safety", 50),
                minimality=qs_data.get("minimality", 50),
                style_consistency=qs_data.get("style_consistency", 50),
            )

            # Parse issues found
            issues_found = []
            for issue_data in data.get("issues_found", []):
                issues_found.append(FixIssue(
                    type=issue_data.get("type", "unknown"),
                    description=issue_data.get("description", ""),
                    line=issue_data.get("line"),
                    severity=issue_data.get("severity", "MEDIUM"),
                    suggestion=issue_data.get("suggestion"),
                ))

            # Determine status
            score = data.get("satisfaction_score", quality_scores.weighted_score)
            status_str = data.get("status", "")

            if status_str:
                try:
                    status = FixChallengerStatus(status_str)
                except ValueError:
                    status = self._score_to_status(score)
            else:
                status = self._score_to_status(score)

            return FixChallengerFeedback(
                iteration=iteration,
                timestamp=datetime.utcnow(),
                satisfaction_score=score,
                threshold=self.threshold,
                status=status,
                quality_scores=quality_scores,
                issues_found=issues_found,
                improvements_needed=data.get("improvements_needed", []),
                positive_feedback=data.get("positive_feedback", []),
                thinking_content=thinking_content,
            )

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse challenger response: {e}")
            # Return minimal feedback on parse error
            return FixChallengerFeedback(
                iteration=iteration,
                satisfaction_score=50,
                threshold=self.threshold,
                status=FixChallengerStatus.NEEDS_IMPROVEMENT,
                quality_scores=FixQualityScores(
                    correctness=50,
                    safety=50,
                    minimality=50,
                    style_consistency=50,
                ),
                improvements_needed=[
                    f"Failed to parse challenger response: {response_text[:200]}"
                ],
            )

    def _score_to_status(self, score: float) -> FixChallengerStatus:
        """Convert satisfaction score to status."""
        if score >= self.threshold:
            return FixChallengerStatus.APPROVED
        elif score >= 50:
            return FixChallengerStatus.NEEDS_IMPROVEMENT
        else:
            return FixChallengerStatus.REJECTED
