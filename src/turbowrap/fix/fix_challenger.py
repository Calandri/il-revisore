"""
Fix Challenger using Gemini CLI.

Evaluates fixes generated by Claude by running in the repo directory
and verifying the actual git diff.

Uses the centralized GeminiCLI class which provides:
- Streaming output via SSE (visible in UI)
- Automatic operation tracking
- S3 artifact saving
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any

from turbowrap.config import get_settings
from turbowrap.fix.models import (
    FixChallengerFeedback,
    FixChallengerStatus,
    FixContext,
    FixQualityScores,
)
from turbowrap.llm import load_prompt
from turbowrap.llm.gemini import GeminiCLI
from turbowrap.review.reviewers.utils.json_extraction import parse_llm_json

logger = logging.getLogger(__name__)

# Timeout for Gemini CLI (3 minutes)
GEMINI_CLI_TIMEOUT = 180


class GeminiFixChallenger:
    """
    Fix challenger using Gemini CLI.

    Runs in the repo directory so Gemini can:
    - Read the actual files
    - Run git diff to verify changes
    - Verify the fix matches what was claimed

    Uses GeminiCLI for streaming output visible in UI.
    """

    def __init__(
        self,
        satisfaction_threshold: float = 95.0,
        timeout: int = GEMINI_CLI_TIMEOUT,
    ):
        """
        Initialize the fix challenger.

        Args:
            satisfaction_threshold: Required score to approve fix (95 default)
            timeout: Timeout in seconds for CLI execution
        """
        self.settings = get_settings()
        self.threshold = satisfaction_threshold
        self.timeout = timeout
        self.model = self.settings.agents.gemini_model

        # Load challenger system prompt from fix_challenger.md (must exist)
        self.system_prompt = load_prompt("fix_challenger")

    async def evaluate_batch(
        self,
        issues: list[FixContext],
        repo_path: Path,
        branch_name: str,
        fixer_output: dict[str, Any] | None = None,
        # Tracking parameters
        repo_name: str | None = None,
        parent_session_id: str | None = None,
    ) -> dict[str, FixChallengerFeedback]:
        """
        Evaluate a batch of fixes using Gemini CLI.

        Args:
            issues: List of fix contexts with issue details
            repo_path: Path to the repository (Gemini runs here)
            branch_name: Current branch name
            fixer_output: Optional fixer's JSON output
            repo_name: Repository name for operation tracking
            parent_session_id: Parent fix session ID for hierarchical tracking

        Returns:
            Dict mapping issue_code -> FixChallengerFeedback
        """
        # Build prompt
        prompt = self._build_batch_prompt(issues, branch_name, fixer_output)

        # Create GeminiCLI instance with streaming support
        gemini = GeminiCLI(
            working_dir=repo_path,
            model=self.model,
            timeout=self.timeout,
            auto_accept=True,
            s3_prefix="fix-challenger",
        )

        logger.info(f"[FIX CHALLENGER] Running Gemini CLI in {repo_path}")

        # Run with automatic operation tracking and streaming
        result = await gemini.run(
            prompt=prompt,
            operation_type="fix",
            repo_name=repo_name or repo_path.name,
            operation_details={
                "parent_session_id": parent_session_id,
                "challenger_type": "gemini-fix",
                "issue_count": len(issues),
                "issue_ids": [ctx.issue_id for ctx in issues],
                "issue_codes": [ctx.issue_code for ctx in issues],
                "branch_name": branch_name,
            },
        )

        # Handle failure
        if not result.success or not result.output:
            logger.error(f"[FIX CHALLENGER] CLI failed: {result.error}")
            return {
                ctx.issue_code: self._create_fallback_feedback(
                    iteration=1,
                    score=50,
                    message=f"Gemini CLI failed: {result.error or 'No output'}",
                )
                for ctx in issues
            }

        logger.info(
            f"[FIX CHALLENGER] Completed in {result.duration_ms}ms, "
            f"output length: {len(result.output)}"
        )

        return self._parse_batch_response(result.output, issues)

    def _build_batch_prompt(
        self,
        issues: list[FixContext],
        branch_name: str,
        fixer_output: dict[str, Any] | None = None,
    ) -> str:
        """Build the evaluation prompt for a batch of issues."""
        # Build issues table
        issues_md = "## Issues to Evaluate\n\n"
        for ctx in issues:
            issues_md += f"""### {ctx.issue_code}

| Field | Value |
|-------|-------|
| **Title** | {ctx.title} |
| **File** | {ctx.file_path} |
| **Line** | {ctx.line or "Unknown"} |
| **Severity** | {ctx.severity} |

**Description**: {ctx.description}

**Original Code**:
```
{ctx.current_code or "Not provided"}
```

**Suggested Fix**:
```
{ctx.suggested_fix or "Not provided"}
```

---

"""

        # Add fixer output if available
        fixer_section = ""
        if fixer_output:
            fixer_section = f"""
## Fixer Output (JSON)

```json
{json.dumps(fixer_output, indent=2)[:5000]}
```

"""

        return f"""{self.system_prompt}

---

{issues_md}

## Branch
Current branch: `{branch_name}`

{fixer_section}

Now run `git diff HEAD` and evaluate each issue. Return the JSON response.
"""

    def _parse_batch_response(
        self,
        response_text: str,
        issues: list[FixContext],
    ) -> dict[str, FixChallengerFeedback]:
        """Parse Gemini CLI batch response."""
        results: dict[str, FixChallengerFeedback] = {}

        try:
            data = parse_llm_json(response_text)
            if not data:
                logger.warning("[FIX CHALLENGER] No JSON found in response")
                # Return fallback for all
                for ctx in issues:
                    results[ctx.issue_code] = self._create_fallback_feedback(
                        iteration=1,
                        score=50,
                        message=f"No JSON in response: {response_text[:200]}",
                    )
                return results

            # Parse per-issue results from "issues" dict
            issues_data = data.get("issues", {})

            for ctx in issues:
                issue_code = ctx.issue_code
                if issue_code in issues_data:
                    issue_result = issues_data[issue_code]
                    results[issue_code] = self._parse_single_issue(issue_result)
                else:
                    # Issue not in response - fallback
                    results[issue_code] = self._create_fallback_feedback(
                        iteration=1,
                        score=50,
                        message=f"Issue {issue_code} not found in challenger response",
                    )

            return results

        except Exception as e:
            logger.error(f"[FIX CHALLENGER] Failed to parse batch response: {e}")
            # Return fallback for all
            for ctx in issues:
                results[ctx.issue_code] = self._create_fallback_feedback(
                    iteration=1,
                    score=50,
                    message=f"Parse error: {e}",
                )
            return results

    def _parse_single_issue(self, issue_data: dict[str, Any]) -> FixChallengerFeedback:
        """Parse a single issue result from the batch response."""
        try:
            score = issue_data.get("score", 50)
            status_str = issue_data.get("status", "")

            # Parse quality scores
            qs_data = issue_data.get("quality_scores", {})
            quality_scores = FixQualityScores(
                correctness=qs_data.get("correctness", score),
                safety=qs_data.get("safety", score),
                minimality=qs_data.get("minimality", score),
                style_consistency=qs_data.get("style", score),
            )

            # Determine status
            if status_str:
                if status_str == "SOLVED":
                    status = FixChallengerStatus.APPROVED
                elif status_str == "IN_PROGRESS":
                    status = FixChallengerStatus.NEEDS_IMPROVEMENT
                else:
                    status = self._score_to_status(score)
            else:
                status = self._score_to_status(score)

            return FixChallengerFeedback(
                iteration=1,
                timestamp=datetime.utcnow(),
                satisfaction_score=score,
                threshold=self.threshold,
                status=status,
                quality_scores=quality_scores,
                issues_found=[],  # Could parse from issue_data if needed
                improvements_needed=issue_data.get("improvements_needed", []),
                positive_feedback=[issue_data.get("feedback", "")]
                if issue_data.get("feedback")
                else [],
            )

        except Exception as e:
            logger.warning(f"[FIX CHALLENGER] Failed to parse issue: {e}")
            return self._create_fallback_feedback(
                iteration=1,
                score=50,
                message=f"Parse error: {e}",
            )

    def _score_to_status(self, score: float) -> FixChallengerStatus:
        """Convert satisfaction score to status."""
        if score >= self.threshold:
            return FixChallengerStatus.APPROVED
        if score >= 50:
            return FixChallengerStatus.NEEDS_IMPROVEMENT
        return FixChallengerStatus.REJECTED

    def _create_fallback_feedback(
        self,
        iteration: int = 1,
        score: int = 50,
        message: str = "Gemini CLI unavailable",
    ) -> FixChallengerFeedback:
        """Create fallback feedback when CLI fails."""
        return FixChallengerFeedback(
            iteration=iteration,
            timestamp=datetime.utcnow(),
            satisfaction_score=score,
            threshold=self.threshold,
            status=FixChallengerStatus.NEEDS_IMPROVEMENT,
            quality_scores=FixQualityScores(
                correctness=score,
                safety=score,
                minimality=score,
                style_consistency=score,
            ),
            improvements_needed=[message] if message else [],
        )
